{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "319c83df",
   "metadata": {},
   "source": [
    "# NLP Lab 2 / Text classification\n",
    "\n",
    "You may work by pairs. There is no increase of grade or compensation if you work alone. \n",
    "Please first indicate whom you work with in the cell above:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f09f2",
   "metadata": {},
   "source": [
    "Your answer here : worked with Matthieu Sousa Ferreira & Baptiste Rozan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e2289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ade3b1",
   "metadata": {},
   "source": [
    "# 0. Submission instructions\n",
    "\n",
    "The due date of this lab is October 13h, 23:59. Late deliveries will be penalized 1pt/day. \n",
    "\n",
    "Please visit https://mvproxy.esiee.fr (disregard the security warning, the certificate is self-signed). When visiting for the first time, provide your ESIEE login, but leave the password field empty, and click on \"Connexion\". You should then receive an email containing your mvproxy password (check your SPAM folder if you don't).\n",
    "\n",
    "Visit https://mvproxy.esiee.fr a second time, but now fill in both your ESIEE login and mvproxy password. You should be logged in.\n",
    "\n",
    "Drop a ZIP archive containing :\n",
    "- This notebook (lab2.ipynb), filled with answers to the questions ;\n",
    "- and a *local* copy of the text articles or CSV files you are working with, which you should access to relatively and not absolutely. \n",
    "\n",
    "Please pay attention to the latter point. Code like\n",
    "```\n",
    "csv.reader('C:\\Users\\Yoyodyne\\My Documents\\AIC-5102B\\Lab2 Text Classification/dataset.csv')\n",
    "```\n",
    "\n",
    "should be banned, and replaced by\n",
    "```\n",
    "csv.reader('dataset.csv')\n",
    "```\n",
    "or\n",
    "```\n",
    "csv.read('./data/class1.csv')\n",
    "csv.read('./data/class2.csv')\n",
    "```\n",
    "\n",
    "I should be able to run your code on *Linux* after sourcing <t>~/pynlp/bin/activate</t> , without modifying the 'C:\\Users\\Yoyodyne\\My Documents\\AIC-5102B' path, which I don't have access to, nor changing anything else in your notebook as I run it. You must also stick to NLTK, and packages I include myself in the following code snippets.\n",
    "\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "This lab must be done on Linux Debian 12, under the Python virtual environment [decribed here](https://perso.esiee.fr/~hilairex/NLP/docker-fc39.html). You may also use the Docker container described in this document if you prefer, but the Python venv is enough.\n",
    "\n",
    "It is all about text classification, and your first task will consist in selecting a dataset.\n",
    "\n",
    "Please visit https://www.kaggle.com/datasets?search=text+classification in order to choose a dataset you like (or randomly, if you don't known which one to choose). We will attempt to separate samples from 2 classes only, so you may either choose between a dataset that has natively 2 classes only (spam/non spam email, positive/negative review, etc.) or one that has natively $n > 2$ classes, but two only of which will be used (e.g. politics/cooking, or sport/computer articles).\n",
    "\n",
    "Which dataset and classes did you chose ? Plesase give your answer below with its related URL, and copy the related files to your working directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4be712-424d-47ae-97b2-a180bf0864ac",
   "metadata": {},
   "source": [
    "Dataset : SMS Spam collection\n",
    "classes : spam / not spam\n",
    "https://www.kaggle.com/datasets/thedevastator/sms-spam-collection-a-more-diverse-dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79abd6",
   "metadata": {},
   "source": [
    "## 2. Text vectorization\n",
    "\n",
    "The following functions :\n",
    "- extract the vocabulary from a CSV file assuming the text is located in column number 5\n",
    "- build the document-term matrix by reading again the same CSV file\n",
    "\n",
    "Adapt them, so that they fit your dataset and produce a document-term matrix in the end.\n",
    "\n",
    "Please note that:\n",
    "- the tokenization method used is wordpunct_tokenize(), which may not be optimal. You may call something different in case you find too much garbage in your resulting vocabulary.\n",
    "- there are two \"if\" tests in dtmat_from_file which appear unnecessary so far. They are, indeed, because the test samples may include unseen words, which would generate out-of-bounds index. So unseen words are just ignored.\n",
    "- you may also consider lemmatizing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "190415fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=True, reduce_len=False)\n",
    "\n",
    "def voc_from_csv(csvfile):\n",
    "    nlines=0\n",
    "    voc=[]\n",
    "    with open(csvfile, errors='ignore') as file:\n",
    "        reader=csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            nlines=nlines+1\n",
    "            voc.extend(tweet_tokenizer.tokenize(row[0]))\n",
    "    voc=sorted(set(voc))        \n",
    "    return voc,nlines\n",
    "\n",
    "def dtmat_from_csv(csvfile):\n",
    "    voc,rows=voc_from_csv(csvfile)\n",
    "    cols=len(voc)\n",
    "    mat=np.zeros((rows,cols))\n",
    "    d=0\n",
    "    with open(csvfile,  errors='ignore') as file:\n",
    "        reader=csv.reader(file, delimiter=',')\n",
    "        for row in reader:\n",
    "            w= nltk.wordpunct_tokenize(row[0])\n",
    "            X=np.searchsorted(voc,w,side='left')            \n",
    "            for i in range(0,len(w)):\n",
    "                if (X[i] < cols):\n",
    "                    if (w[i] == voc[X[i]]):\n",
    "                        mat[d][X[i]]+=1\n",
    "            d=d+1\n",
    "    return mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "132552a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = 'sms-spam-collection-a-more-diverse-dataset/train.csv'\n",
    "vocab = voc_from_csv(csv_path)\n",
    "dtmat = dtmat_from_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d9a6e",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "Run dtmat_from_csv on your dataset, or on a sample drawn from it if it is very large. Examine the resulting matrix. How many times does it happen that a given word is seen only once (possibly twice) in your training set ? Give a few Python code lines which show this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d99d89ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots vus une seule fois : ['0089', '01223585236', '02072069400', '02085076972', '07008009200', '07046744435', '07090201529', '07090298926', '07099833605', '07732584351', '07753741225', '077xxx', '078', '07801543489', '07808', '07808247860', '07808726822', '07815296484', '078498', '07880867867', '0789xxxxxxx', '07946746291', '0796XXXXXX', '07973788240', '07XXXXXXXXX', '08002988890', '08081263000', '08448350055', '08448714184', '08450542832', '08452810071', '08700469649', '08701213186', '08701237397', '08701752560', '08702490080', '08704050406', '08704439680', '08707500020', '08707808226', '08708800282', '08709501522', '08712103738', '08712400200', '08712400603', '08712402578', '08712402779', '08712402902', '08712402972', '08712404000', '08712466669', '08714342399', '08714712379', '08714712388', '08714712394', '08714712412', '08714714011', '08715203028', '08715203649', '08715203652', '08715203656', '08715203677', '08715203685', '08715203694', '08715205273', '08715500022', '08717111821', '08717168528', '08717205546', '08717507382', '08717509990', '08717890890', '08717895698', '08718711108', '08718723815', '08718725756', '08718726970', '08718726971', '08718726978', '08718730555', '08718738002', '08718738034', '08719180219', '08719181259', '08719181503', '08719839835', '08719899217', '08719899229', '08719899230', '09041940223', '09050000301', '09050000332', '09050000460', '09050000555', '09050000878', '09050000928', '09050001295', '09050005321', '09050280520', '09053750005']\n",
      "Mots vus deux fois : ['01223585334', '02073162414', '050703', '07123456789', '07734396839', '07742676969', '0776xxxxxxx', '07786200117', '07821230901', '07xxxxxxxxx', '08', '08000407165', '08000776320', '08002888812', '08002986030', '08006344447', '08081560665', '0844', '08706091795', '08708034412', '08709222922', '08712101358', '08712317606', '08712402050', '08712405022', '0871750', '08718726270', '08718727868', '08718730666', '08718738001', '08719180248', '08719181513', '09050001808', '09050002311', '09050003091', '09056242159', '09058091854', '09058094565', '09058094597', '09058094599', '09058099801', '09061213237', '09061221061', '09061701461', '09061702893', '09061743386', '09061743806', '09061744553', '09064011000', '09064012160', '09064019014', '09065171142', '09065174042', '09065989182', '09066350750', '09066358152', '09066364311', '09066364589', '09066380611', '09066382422', '09071512433', '09094646899', '1030', '10K', '10ppm', '113', '1250', '150P16', '150PPM', '165', '177', '18p', '1hr', '1win150ppmx3', '2007', '22', '250k', '26th', '28days', '2C', '2End', '2MOROW', '2WT', '2find', '2gthr', '2marrow', '2mrw', '2p', '2rcv', '2stop', '2stoptxt', '2u', '2waxsto', '2wks', '2yr', '300603', '300p', '30pp', '3100', '31p']\n",
      "Nombre de mots vus une seule fois : 5252\n",
      "Nombre de mots vus deux fois : 1711\n"
     ]
    }
   ],
   "source": [
    "# Calculer la fréquence totale de chaque mot dans le corpus\n",
    "total_counts = np.sum(dtmat, axis=0)\n",
    "\n",
    "# Indices des mots vus une seule fois\n",
    "once_indices = np.where(total_counts == 1)[0]\n",
    "# Indices des mots vus deux fois\n",
    "twice_indices = np.where(total_counts == 2)[0]\n",
    "\n",
    "# Afficher quelques exemples de mots vus une seule ou deux fois\n",
    "print(\"Mots vus une seule fois :\", [vocab[0][i] for i in once_indices[:100]])\n",
    "print(\"Mots vus deux fois :\", [vocab[0][i] for i in twice_indices[:100]])\n",
    "\n",
    "# Nombre total de mots vus une seule fois\n",
    "print(\"Nombre de mots vus une seule fois :\", len(once_indices))\n",
    "# Nombre total de mots vus deux fois\n",
    "print(\"Nombre de mots vus deux fois :\", len(twice_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d66ec46-4fb7-41ff-acb1-b2eed3b73d10",
   "metadata": {},
   "source": [
    "### your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b57a54-94a4-4a1a-9d1a-805d99442bdd",
   "metadata": {},
   "source": [
    "### Strategy Choice (Question 2)\n",
    "\n",
    "We adopt **Strategy 2**, the kernelized LDA with the linear kernel \\(k(x, y)=\\langle x,y\r",
    "angle\\).\n",
    "The dataset contains 5 574 SMS with ~8.9 k unique tokens, so a dense covariance\n",
    "in the original vocabulary space would be singular (\\(d \\gg n\\)) and expensive to invert.\n",
    "Working in the sample space through the Gram matrix circumvents that issue while keeping\n",
    "memory within reach (≈250 MB in float64, much less in float32), and lets us rely on\n",
    "iterative eigensolvers provided by `kfda`/`scikit-learn`.\n",
    "We will therefore stick to strategy 2 for the implementation that follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c238cffb-61b3-4248-94be-d628a23bc875",
   "metadata": {},
   "source": [
    "### Your answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69142056-bfd2-4cab-9595-83e297a777b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd9235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# Helper utilities to get bag-of-words features and labels once for the whole notebook\n",
    "\n",
    "def load_sms_dataset(csv_path):\n",
    "    texts, labels = [], []\n",
    "    with open(csv_path, newline='', encoding='utf-8', errors='ignore') as fh:\n",
    "        reader = csv.DictReader(fh)\n",
    "        for row in reader:\n",
    "            texts.append(row['sms'])\n",
    "            labels.append(int(row['label']))\n",
    "    return texts, np.array(labels, dtype=np.int32)\n",
    "\n",
    "\n",
    "def build_sparse_bow(texts, tokenizer=tweet_tokenizer):\n",
    "    indptr = [0]\n",
    "    indices, data = [], []\n",
    "    vocab = {}\n",
    "    for text in texts:\n",
    "        token_counts = Counter(tokenizer.tokenize(text))\n",
    "        for token, count in token_counts.items():\n",
    "            idx = vocab.setdefault(token, len(vocab))\n",
    "            indices.append(idx)\n",
    "            data.append(float(count))\n",
    "        indptr.append(len(indices))\n",
    "    matrix = csr_matrix((data, indices, indptr), shape=(len(texts), len(vocab)), dtype=np.float32)\n",
    "    return matrix, vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2116dab",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Implement the strategy you chose at question 2. You may either use the sklearn.lda.LDA class from Scikit-Learn (strategy 1), the Kfda class from Pypi (strategy 2), or your own implementation (strategy 3). In all cases, you should assume that the data of the two classes are normally distributed after they are projected.\n",
    "Test your implementation on the classes you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e472e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc15f6",
   "metadata": {},
   "source": [
    "## 4. Kernelized LDA\n",
    "\n",
    "The kernelized version of LDA is implemented as a kfda package. Its homepage is here: https://pypi.org/project/kfda/\n",
    "To install it, suffice to run <t>pip3 install kfda</t> from your Python virtual environment.\n",
    "\n",
    "### Question 6\n",
    "\n",
    "Let $\\boldsymbol{x}$ and $\\boldsymbol{y}$ be any two columns of your D-T matrix (which you may assume TD-IDF normalized or not, it does not change the problem). Consider the inhomogeneous polynomial kernel \n",
    "$$k(\\boldsymbol{x},\\boldsymbol{y})= (1+<\\boldsymbol{x},\\boldsymbol{y}>)^d$$\n",
    "where $d>0$ is integer.\n",
    "\n",
    "- Suppose that $d=2$, and that the above kernel is used in a kernelized LDA. What are the new axes created in the feature space, that didn't exist when $d=1$? Which of these could be useful, and change the solution computed by LDA in the feature space ?\n",
    "- If the input data consists of $n$ samples, what is the time complexity of K-FDA ? \n",
    "- Try to classify using this setup, and report your results. Then increase $n$, from 2 to 4. You will very likely encounter some overflow issues. If it happens, explain what is wrong, and add some code to preprocess the data to circumvent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435382c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fb3b61",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "We will now slightely improve the above kernel by replacing the natural dot product \n",
    "$$<\\boldsymbol{x},\\boldsymbol{y}>$$ \n",
    "by \n",
    "$$ \\sum_i \\min(\\boldsymbol{x}_i, \\boldsymbol{y}_i) $$\n",
    "resulting in\n",
    "\n",
    "$$f(\\boldsymbol{x},\\boldsymbol{y})= (1+\\sum_i \\min(\\boldsymbol{x}_i, \\boldsymbol{y}_i) )^n$$\n",
    "\n",
    "Is $f$ a positive semidefinite kernel ? Either prove that it is, or give a counter-example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2450b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45996a2d",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "Irrespective of your answer to question 8, try kfda with $f$ as its kernel. Looking at the source code https://github.com/concavegit/kfda/blob/master/kfda/kfda.py you will notice (line 92) that it relies on the paiwise_kernels function from sklearn to compute the Gram matrix. \n",
    "\n",
    "According to sklearn documentation https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_kernels.html the kernel parameter can be a callable, hence you can supply a function of your own as the kernel argument, possibly using the keywords field (kwds).\n",
    "\n",
    "Report your classification results, possibly varying $n$ (be reasonable with values, high $n$ may cause floating point exceptions, as in the last question). You should likely obtain decent (~ 75% accuracy, say) but not outstanding results.  This, however, is highly dependent on the dataset and classes you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42178c26",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "One reason why the obtained accuracy is not fantastic is that the vector model we are using is blind to bigrams. For instance, we may encounter (normalized) words \"donald\", and \"trump\" separately in a document, but this is very different from \"donald trump\".\n",
    "\n",
    "One way to fix this is to include bigrams in the vocabulary : for two consecutive words, like \"donald trump\", we would add a synthetic word \"donald_trump\" to the vocabulary. \n",
    "\n",
    "Add an extra \"bigram\" parameter to voc_from_csv() to do so, and compare your results to those of question 9. Bigrams can be generated very simply using a code similar to this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fb98f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I_think',\n",
       " 'think_traveling',\n",
       " 'traveling_to',\n",
       " 'to_Rio',\n",
       " 'Rio_next',\n",
       " 'next_winter',\n",
       " 'winter_would',\n",
       " 'would_be',\n",
       " 'be_great']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=['I','think','traveling','to','Rio','next','winter','would','be','great']\n",
    "[w[i]+'_'+w[i+1] for i in range(0,len(w)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b12aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
